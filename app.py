import re
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import streamlit as st

from transformers import (
    AutoTokenizer,
    AutoConfig,
    AutoModel,
    PreTrainedModel,
    AutoModelForCausalLM,
)

torch.set_grad_enabled(False)

# =========================
# Engine 1: Desklib DeBERTa AI Detector (è‹±æ–‡)
# =========================

DET_MODEL_NAME = "desklib/ai-text-detector-v1.01"


class DesklibAIDetectionModel(PreTrainedModel):
    """
    ä¾å®˜æ–¹ model card å¯«çš„ wrapperï¼š
    - base: DeBERTa-v3
    - classifier: ç·šæ€§å±¤è¼¸å‡º 1 ç¶­ logit (AI-generated)
    """
    config_class = AutoConfig

    def __init__(self, config):
        super().__init__(config)
        self.model = AutoModel.from_config(config)
        self.classifier = nn.Linear(config.hidden_size, 1)
        self.init_weights()

    def forward(self, input_ids, attention_mask=None, labels=None):
        outputs = self.model(input_ids, attention_mask=attention_mask)
        last_hidden_state = outputs[0]                       # [B, T, H]

        input_mask_expanded = attention_mask.unsqueeze(-1).expand(
            last_hidden_state.size()
        ).float()
        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, dim=1)
        sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)
        pooled_output = sum_embeddings / sum_mask            # masked mean pooling

        logits = self.classifier(pooled_output)              # [B, 1]

        loss = None
        if labels is not None:
            loss_fct = nn.BCEWithLogitsLoss()
            loss = loss_fct(logits.view(-1), labels.float())

        output = {"logits": logits}
        if loss is not None:
            output["loss"] = loss
        return output


@st.cache_resource
def load_detector():
    config = AutoConfig.from_pretrained(DET_MODEL_NAME)
    tokenizer = AutoTokenizer.from_pretrained(DET_MODEL_NAME)
    model = DesklibAIDetectionModel.from_pretrained(DET_MODEL_NAME, config=config)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()
    return tokenizer, model, device


def detector_predict_prob_batch(
    sent_list, tokenizer, model, device, max_len: int = 256
):
    """
    ä¸€æ¬¡å°å¤šå€‹å¥å­åš batch æ¨è«–ï¼Œå›å‚³ AI æ©Ÿç‡ listï¼ˆ0~1ï¼‰ã€‚
    ç©ºåˆ—è¡¨ â†’ å›ç©º listã€‚
    """
    if not sent_list:
        return []

    encoded = tokenizer(
        sent_list,
        padding=True,
        truncation=True,
        max_length=max_len,
        return_tensors="pt",
    )
    input_ids = encoded["input_ids"].to(device)
    attention_mask = encoded["attention_mask"].to(device)

    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs["logits"].view(-1)  # [B]

    probs_ai = torch.sigmoid(logits).cpu().numpy().tolist()
    return probs_ai


def detector_predict_prob_doc(text: str, tokenizer, model, device, max_len: int = 512):
    """
    æ•´ç¯‡æ–‡ç« ä¸Ÿä¸€æ¬¡æ¨¡å‹ï¼Œå›å‚³ (ai_prob, human_prob)ã€‚
    """
    if not text.strip():
        return 0.5, 0.5

    encoded = tokenizer(
        text,
        padding="max_length",
        truncation=True,
        max_length=max_len,
        return_tensors="pt",
    )
    input_ids = encoded["input_ids"].to(device)
    attention_mask = encoded["attention_mask"].to(device)

    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs["logits"]  # [1,1]
        ai_prob = torch.sigmoid(logits).item()

    ai_prob = float(ai_prob)
    human_prob = 1.0 - ai_prob
    return ai_prob, human_prob


# =========================
# Engine 2: GPT-2 Perplexity (è‹±æ–‡)
# =========================

LM_MODEL_NAME = "distilgpt2"  # æ¯” gpt2 å°ä¸€é»ï¼Œè¼ƒå¿«


@st.cache_resource
def load_language_model():
    tok = AutoTokenizer.from_pretrained(LM_MODEL_NAME)
    model = AutoModelForCausalLM.from_pretrained(LM_MODEL_NAME)
    if tok.pad_token is None:
        tok.pad_token = tok.eos_token
        model.config.pad_token_id = tok.eos_token_id
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()
    return tok, model, device


def compute_perplexity(text: str, tok, model, device, max_length: int = 256):
    """
    ä½¿ç”¨ GPT-2 è¨ˆç®— cross-entropy loss èˆ‡ perplexityã€‚
    åªå–å‰ max_length tokenã€‚
    """
    if not text.strip():
        return 0.0, float("inf")

    enc = tok(
        text,
        return_tensors="pt",
        truncation=True,
        max_length=max_length,
    )
    input_ids = enc["input_ids"].to(device)

    with torch.no_grad():
            outputs = model(input_ids, labels=input_ids)
            loss = outputs.loss

    loss_val = loss.item()
    ppl = float(torch.exp(loss))
    return loss_val, ppl


def heuristic_ai_from_ppl(ppl: float) -> float:
    """
    æ ¹æ“š perplexity çµ¦ä¸€å€‹ç²—ç•¥ AI æ©Ÿç‡ (0~1)ï¼š
      - ppl â‰¤ 20ï¼šéå¸¸å¥½é æ¸¬ â†’ å¾ˆåƒ AI â†’ ~0.9
      - ppl â‰¥ 80ï¼šå¾ˆé›£é æ¸¬ â†’ å¾ˆåƒ Human â†’ ~0.1
      - ä¸­é–“ç·šæ€§æ’å€¼
    ç´”å•Ÿç™¼å¼ï¼Œç•¶ã€Œç¬¬äºŒæ„è¦‹ã€ç”¨ã€‚
    """
    if ppl <= 20:
        ai_prob = 0.9
    elif ppl >= 80:
        ai_prob = 0.1
    else:
        ai_prob = 0.9 - (ppl - 20) * (0.8 / 60.0)
    return max(0.0, min(1.0, ai_prob))


# =========================
# Stylometry + utils
# =========================

def split_sentences_en(text: str):
    """
    å¾ˆç°¡å–®çš„è‹±æ–‡æ–·å¥ï¼šä»¥ . ? ! å’Œæ›è¡Œç‚ºä¸»ã€‚
    """
    parts = re.split(r"[\.!?ã€‚\n\r]+", text)
    sents = [p.strip() for p in parts if p.strip()]
    return sents


def tokenize_for_stats(text: str):
    """
    çµ¦ Stylometry ç”¨çš„ tokenã€‚è‹±æ–‡æƒ…å¢ƒï¼š\w+ æŠ“è‹±æ–‡å–®å­—ã€‚
    """
    tokens = re.findall(r"\w+", text)
    return tokens


def compute_stylometry(text: str):
    chars = [c for c in text if not c.isspace()]
    n_chars = len(chars)

    sentences = split_sentences_en(text)
    sent_lens = [len(s) for s in sentences]
    avg_len = float(np.mean(sent_lens)) if sent_lens else 0.0
    std_len = float(np.std(sent_lens)) if sent_lens else 0.0
    burstiness = std_len / avg_len if avg_len > 0 else 0.0

    tokens = tokenize_for_stats(text)
    ttr = len(set(tokens)) / len(tokens) if tokens else 0.0

    puncts = re.findall(r"[^\w\s]", text)
    noise_ratio = len(puncts) / n_chars if n_chars > 0 else 0.0

    human_noise_patterns = [
        r"haha",
        r"lol",
        r"XD",
        r"\?{2,}",        # å…©å€‹ä»¥ä¸Š ?
        r"!{2,}",        # å…©å€‹ä»¥ä¸Š !
        r"[ğŸ˜‚ğŸ¤£ğŸ˜…ğŸ˜­ğŸ˜ğŸ¤”]",
    ]
    human_noise_hits = sum(
        len(re.findall(p, text, flags=re.IGNORECASE))
        for p in human_noise_patterns
    )

    return {
        "Characters": n_chars,
        "Sentences": len(sentences),
        "Avg sentence length": avg_len,
        "Sentence length std": std_len,
        "Burstiness (Ïƒ/Î¼)": burstiness,
        "Type-Token Ratio (TTR)": ttr,
        "Punctuation ratio": noise_ratio,
        "Human-noise count": human_noise_hits,
    }


def stylometry_ai_score(feat):
    """
    æ ¹æ“š stylometry ç‰¹å¾µè¼¸å‡º AI æ©Ÿç‡ (0~1)
    """
    burst = feat["Burstiness (Ïƒ/Î¼)"]
    ttr = feat["Type-Token Ratio (TTR)"]
    noise = feat["Human-noise count"]

    # Burstiness scoreï¼ˆä½ â†’ AIï¼‰
    if burst <= 0.3:
        s_b = 1.0
    elif burst >= 0.8:
        s_b = 0.0
    else:
        s_b = 1 - (burst - 0.3) / (0.8 - 0.3)

    # TTR scoreï¼ˆä½ â†’ AIï¼‰
    if ttr <= 0.35:
        s_t = 1.0
    elif ttr >= 0.7:
        s_t = 0.0
    else:
        s_t = 1 - (ttr - 0.35) / (0.7 - 0.35)

    # Noise scoreï¼ˆå°‘ â†’ AIï¼‰
    if noise == 0:
        s_n = 1.0
    elif noise >= 3:
        s_n = 0.0
    else:
        s_n = 1 - (noise / 3)

    # æ¬Šé‡ï¼ˆå¯èª¿ï¼‰
    w1, w2, w3 = 0.4, 0.3, 0.3
    ai_score = (w1*s_b + w2*s_t + w3*s_n) / (w1 + w2 + w3)

    return float(ai_score)




def color_for_ai_prob(p: float, low_thr: float, high_thr: float) -> str:
    """
    æ ¹æ“š AI æ©Ÿç‡ä¸Šè‰²ï¼š
      p >= high_thr â†’ ç´…
      p <= low_thr  â†’ ç¶ 
      å…¶ä»–         â†’ é»ƒ
    """
    if p >= high_thr:
        return "#ffb3b3"
    elif p <= low_thr:
        return "#b3ffb3"
    else:
        return "#fff4b3"


def render_colored_sentences(sent_df: pd.DataFrame, low_thr: float, high_thr: float):
    html_parts = []
    for _, row in sent_df.iterrows():
        p_ai = row["AI_prob"]
        sent = row["Sentence"]
        color = color_for_ai_prob(p_ai, low_thr, high_thr)
        html_parts.append(
            f"<span style='background-color:{color};"
            f"padding:2px 4px;margin:2px;display:inline-block;'>"
            f"[AI {p_ai*100:.1f}%] {sent}</span>"
        )
    html = "<div style='line-height:1.8;'>{}</div>".format("<br>".join(html_parts))
    st.markdown(html, unsafe_allow_html=True)


# =========================
# Streamlit App
# =========================

st.set_page_config(
    page_title="AI vs Human Detector (Multi-Engine)",
    layout="wide",
)

st.title("ğŸ¤– AI vs Human æ–‡ç« åµæ¸¬å™¨ï¼ˆå¯é¸å¼•æ“ç‰ˆï¼‰")

st.markdown(
    """
æœ¬å·¥å…·æä¾›ä¸‰ç¨®åˆ†æå±¤ç´šï¼Œ**åªæœ‰å‹¾é¸çš„å¼•æ“æ‰æœƒè¼‰å…¥æ¨¡å‹**ï¼š

- ğŸŸ¢ **Stylometryï¼ˆé è¨­å•Ÿç”¨ï¼‰**ï¼š  
  ä¸ä¸‹è¼‰ä»»ä½•æ¨¡å‹ï¼Œåªè¨ˆç®—æ–‡å­—çµ±è¨ˆç‰¹å¾µï¼Œé€Ÿåº¦æœ€å¿«ã€‚

- ğŸ”µ **Engine 1 â€” DeBERTa AI Detector**ï¼ˆ`desklib/ai-text-detector-v1.01`ï¼‰ï¼š  
  é‡å° **è‹±æ–‡æ–‡æœ¬** çš„ AI / Human åˆ†é¡å™¨ã€‚

- ğŸŸ£ **Engine 2 â€” GPT-2 Perplexity**ï¼ˆ`distilgpt2`ï¼‰ï¼š  
  ä½¿ç”¨èªè¨€æ¨¡å‹å›°æƒ‘åº¦ (Perplexity) ç²—ç•¥ä¼°è¨ˆ AI å¯èƒ½æ€§ï¼ˆç¬¬äºŒæ„è¦‹ï¼‰ã€‚

> âš ï¸ DeBERTa èˆ‡ GPT-2 éƒ½æ˜¯é‡å°è‹±æ–‡è¨“ç·´ï¼Œç”¨åœ¨ä¸­æ–‡æˆ–æ··åˆæ–‡æœ¬æ™‚æº–ç¢ºåº¦æœƒä¸‹é™ã€‚  
> æ‰€æœ‰çµæœåƒ…ä¾›åƒè€ƒï¼Œä¸æ‡‰ä½œç‚ºå­¸è¡“é•è¦æˆ–æ³•å¾‹åˆ¤å®šçš„å”¯ä¸€ä¾æ“šã€‚
"""
)

# ---- Sidebar: engine selection ----
st.sidebar.header("âš™ï¸ Engine é¸æ“‡")
use_stylometry = st.sidebar.checkbox("Stylometryï¼ˆæ–‡å­—çµ±è¨ˆï¼Œæœ€å¿«ï¼‰", value=True)
use_deberta = st.sidebar.checkbox("Engine 1 â€” DeBERTa AI Detector", value=False)
use_gpt = st.sidebar.checkbox("Engine 2 â€” GPT-2 Perplexity", value=False)

st.sidebar.header("ğŸšï¸ DeBERTa åˆ¤æ–·é–¾å€¼")
low_thr = st.sidebar.slider("Human-like ä¸Šé™ï¼ˆAI æ©Ÿç‡ â‰¤ï¼‰", 0.0, 0.5, 0.3, 0.05)
high_thr = st.sidebar.slider("AI-like ä¸‹é™ï¼ˆAI æ©Ÿç‡ â‰¥ï¼‰", 0.5, 1.0, 0.7, 0.05)

if high_thr <= low_thr:
    st.sidebar.warning("âš ï¸ å»ºè­° AI-like é–¾å€¼è¦å¤§æ–¼ Human-like é–¾å€¼ã€‚")

# ---- Lazy load engines ----
det_tokenizer = det_model = det_device = None
lm_tok = lm_model = lm_device = None

if use_deberta:
    with st.spinner("è¼‰å…¥ Engine 1ï¼ˆDeBERTa AI Detectorï¼‰..."):
        det_tokenizer, det_model, det_device = load_detector()
    st.success("Engine 1 è¼‰å…¥å®Œæˆï¼šdesklib/ai-text-detector-v1.01")

if use_gpt:
    with st.spinner("è¼‰å…¥ Engine 2ï¼ˆGPT-2 Perplexityï¼‰..."):
        lm_tok, lm_model, lm_device = load_language_model()
    st.success(f"Engine 2 è¼‰å…¥å®Œæˆï¼š{LM_MODEL_NAME}")

# ---- Text input ----
text = st.text_area(
    "âœï¸ è«‹è¼¸å…¥è¦åµæ¸¬çš„æ–‡ç« ï¼ˆå»ºè­°è‹±æ–‡ï¼‰ï¼š",
    height=220,
    placeholder="Paste an English paragraph (e.g., essay, report, blog post)...",
)

if text.strip():
    sentences = split_sentences_en(text)

    # =========================
    # Engine 1: DeBERTa (doc-level + sentence-level)
    # =========================
    if use_deberta and det_tokenizer is not None:
        st.subheader("ğŸ“Œ Engine 1ï¼šæ•´ç¯‡æ–‡ç« åˆ¤æ–·çµæœï¼ˆDeBERTa AI Detectorï¼‰")

        ai_prob_doc, human_prob_doc = detector_predict_prob_doc(
            text, det_tokenizer, det_model, det_device
        )
        c1, c2 = st.columns(2)
        with c1:
            st.metric("AI æ©Ÿç‡ï¼ˆæ•´ç¯‡ï¼‰", f"{ai_prob_doc * 100:.1f}%")
            st.progress(ai_prob_doc)
        with c2:
            st.metric("Human æ©Ÿç‡ï¼ˆæ•´ç¯‡ï¼‰", f"{human_prob_doc * 100:.1f}%")
            st.progress(human_prob_doc)

        st.subheader("ğŸ” Engine 1ï¼šå¥å­ç´šåˆ¥ AI åµæ¸¬")

        ai_probs_sent = detector_predict_prob_batch(
            sentences, det_tokenizer, det_model, det_device
        )

        sent_rows = []
        for idx, (s, a_p) in enumerate(zip(sentences, ai_probs_sent), start=1):
            if a_p >= high_thr:
                label = "AI-like"
            elif a_p <= low_thr:
                label = "Human-like"
            else:
                label = "Uncertain"
            sent_rows.append(
                {
                    "Index": idx,
                    "Sentence": s,
                    "AI_prob": a_p,
                    "Human_prob": 1.0 - a_p,
                    "Label": label,
                    "Length": len(s),
                }
            )

        if sent_rows:
            sent_df = pd.DataFrame(sent_rows)

            # é¡å‹æ¯”ä¾‹
            st.markdown("**ğŸ“Š å¥å­é¡å‹æ¯”ä¾‹ï¼ˆEngine 1ï¼‰**")
            type_counts = (
                sent_df["Label"]
                .value_counts()
                .reindex(["AI-like", "Uncertain", "Human-like"])
                .fillna(0)
                .astype(int)
            )
            total_sents = len(sent_df)
            ratio_df = pd.DataFrame(
                {
                    "Type": type_counts.index,
                    "Count": type_counts.values,
                    "Ratio": [
                        f"{c/total_sents*100:.1f}%"
                        if total_sents > 0
                        else "0.0%"
                        for c in type_counts.values
                    ],
                }
            )
            st.table(ratio_df)

            # è‡ªç„¶èªè¨€æ‘˜è¦
            st.markdown("**ğŸ“ è‡ªç„¶èªè¨€æ‘˜è¦ï¼ˆEngine 1ï¼Œå¯è²¼åˆ°å ±å‘Šï¼‰**")
            ai_like_cnt = int(type_counts.get("AI-like", 0))
            human_like_cnt = int(type_counts.get("Human-like", 0))
            uncertain_cnt = int(type_counts.get("Uncertain", 0))
            avg_ai_sent = sent_df["AI_prob"].mean() if total_sents > 0 else 0.0
            max_row = (
                sent_df.loc[sent_df["AI_prob"].idxmax()] if total_sents > 0 else None
            )

            summary_lines = []
            summary_lines.append(
                f"- æ•´ç¯‡æ–‡å­—çš„ AI æ©Ÿç‡ç´„ç‚º **{ai_prob_doc*100:.1f}%**ï¼ŒHuman æ©Ÿç‡ç´„ç‚º **{human_prob_doc*100:.1f}%**ã€‚"
            )
            summary_lines.append(
                f"- å…±æœ‰ **{total_sents} å¥**ï¼Œå…¶ä¸­ AI-likeï¼š**{ai_like_cnt}**ï¼ŒHuman-likeï¼š**{human_like_cnt}**ï¼Œä¸ç¢ºå®šï¼š**{uncertain_cnt}**ã€‚"
                f"ï¼ˆé–¾å€¼ï¼šAI â‰¥ {high_thr:.2f}ã€Human â‰¤ {low_thr:.2f}ï¼‰"
            )
            summary_lines.append(
                f"- å¥å­å¹³å‡ AI æ©Ÿç‡ç´„ç‚º **{avg_ai_sent*100:.1f}%**ã€‚"
            )
            if max_row is not None:
                summary_lines.append(
                    f"- AI æ©Ÿç‡æœ€é«˜çš„å¥å­æ˜¯ç¬¬ **{int(max_row['Index'])} å¥**ï¼ˆç´„ **{max_row['AI_prob']*100:.1f}%**ï¼‰ï¼š"
                    f"ã€Œ{max_row['Sentence'][:120]}{'...' if len(max_row['Sentence'])>120 else ''}ã€ã€‚"
                )
            st.markdown("\n".join(summary_lines))

            st.markdown("**ğŸ“‹ å¥å­æ¸…å–®ï¼ˆå¯æ’åºï¼‰**")
            st.dataframe(
                sent_df[["Index", "Label", "AI_prob", "Human_prob", "Sentence"]],
                use_container_width=True,
            )

            st.markdown("**ğŸ“Š æ¯å¥ AI æ©Ÿç‡ï¼ˆbar chartï¼‰**")
            chart_df = sent_df.set_index("Index")[["AI_prob"]]
            st.bar_chart(chart_df)

            st.markdown("**ğŸ“Š å¥é•· vs AI æ©Ÿç‡ï¼ˆscatterï¼‰**")
            scatter_df = sent_df[["Length", "AI_prob"]]
            st.scatter_chart(scatter_df)

            st.markdown("**ğŸ¨ å¥å­è¦–è¦ºåŒ–ï¼ˆèƒŒæ™¯è‰²ä»£è¡¨ AI å¯èƒ½æ€§ï¼‰**")
            st.caption(
                f"AI-likeï¼ˆAIâ‰¥{high_thr:.2f}ï¼‰= ç´…ã€Human-likeï¼ˆAIâ‰¤{low_thr:.2f}ï¼‰= ç¶ ã€ä¸­é–“ = é»ƒã€‚"
            )
            render_colored_sentences(sent_df, low_thr=low_thr, high_thr=high_thr)
        else:
            st.info("ç„¡æ³•åˆ‡å‡ºå¥å­ï¼Œè«‹ç¢ºèªæ–‡å­—å…§å®¹æ˜¯å¦æ­£ç¢ºã€‚")

    elif use_deberta:
        st.warning("Engine 1 é¸æ“‡äº†ï¼Œä½†æ¨¡å‹å°šæœªè¼‰å…¥æˆåŠŸï¼Ÿè«‹é‡æ–°æ•´ç†è©¦è©¦ã€‚")

    # =========================
    # Engine 2: GPT-2 Perplexity
    # =========================
    if use_gpt and lm_tok is not None:
        st.subheader("ğŸ“‰ Engine 2ï¼šPerplexity åˆ†æï¼ˆGPT-2ï¼‰")
        loss_val, ppl = compute_perplexity(
            text, lm_tok, lm_model, lm_device, max_length=256
        )
        ai_prob_ppl = heuristic_ai_from_ppl(ppl)
        human_prob_ppl = 1.0 - ai_prob_ppl

        c3, c4, c5 = st.columns(3)
        with c3:
            st.metric("Cross-Entropy Loss", f"{loss_val:.3f}")
        with c4:
            st.metric("Perplexity (PP)", f"{ppl:.1f}")
        with c5:
            st.metric("AI æ©Ÿç‡ï¼ˆä¾ PP æ¨ä¼°ï¼‰", f"{ai_prob_ppl * 100:.1f}%")
        st.progress(ai_prob_ppl)
        st.caption(
            "ç›´è¦ºï¼šPerplexity è¶Šä½ â†’ è¶Šåƒæ¨¡å‹è‡ªå·±å¯«ï¼ˆAIï¼‰ï¼›"
            "Perplexity è¶Šé«˜ â†’ è¶Šåƒäººé¡è‡ªç„¶èªè¨€ã€‚æ­¤ç‚ºå•Ÿç™¼å¼ä¼°è¨ˆã€‚"
        )
    elif use_gpt:
        st.warning("Engine 2 é¸æ“‡äº†ï¼Œä½†æ¨¡å‹å°šæœªè¼‰å…¥æˆåŠŸï¼Ÿè«‹é‡æ–°æ•´ç†è©¦è©¦ã€‚")

    # =========================
    # Stylometry
    # =========================
    if use_stylometry:
        st.subheader("ğŸ“ˆ Stylometryï¼šæ–‡æœ¬çµ±è¨ˆç‰¹å¾µ")
        feat = compute_stylometry(text)
        feat_df = pd.DataFrame.from_dict(feat, orient="index", columns=["Value"])
        st.table(feat_df)

        # Stylometry AI Score
        ai_prob_style = stylometry_ai_score(feat)
        human_prob_style = 1 - ai_prob_style

        st.markdown("### ğŸ” Stylometry AI åˆ¤æ–·çµæœ")
        c1, c2 = st.columns(2)
        with c1:
            st.metric("AI æ©Ÿç‡ï¼ˆStylometryï¼‰", f"{ai_prob_style*100:.1f}%")
            st.progress(ai_prob_style)
        with c2:
            st.metric("Human æ©Ÿç‡ï¼ˆStylometryï¼‰", f"{human_prob_style*100:.1f}%")
            st.progress(human_prob_style)

        # bar chart
        st.markdown("**ğŸ” Stylometry ç‰¹å¾µåœ–**")
        numeric_feat_df = feat_df[
            feat_df["Value"].apply(lambda x: isinstance(x, (int, float)))
        ]
        st.bar_chart(numeric_feat_df)


else:
    st.info("è«‹åœ¨ä¸Šæ–¹è¼¸å…¥ä¸€æ®µè‹±æ–‡æ–‡å­—ï¼Œå†é¸æ“‡å·¦å´è¦å•Ÿç”¨çš„å¼•æ“ï¼Œæˆ‘æœƒå¹«ä½ åš AI / Human åˆ†æã€‚")
